{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "325398ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting termcolor\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Building wheels for collected packages: termcolor\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4848 sha256=1bc991ab48633f27fbe6d36eb8b99c79b226717aa8c48d2283bfca0cc2edb718\n",
      "  Stored in directory: c:\\users\\visha\\appdata\\local\\pip\\cache\\wheels\\b6\\0d\\90\\0d1bbd99855f99cb2f6c2e5ff96f8023fad8ec367695f7d72d\n",
      "Successfully built termcolor\n",
      "Installing collected packages: termcolor\n",
      "Successfully installed termcolor-1.1.0\n"
     ]
    }
   ],
   "source": [
    "! pip install termcolor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e76efb45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-1.6.1-py3-none-win_amd64.whl (125.4 MB)\n",
      "Requirement already satisfied: scipy in c:\\users\\visha\\anaconda3\\lib\\site-packages (from xgboost) (1.7.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\visha\\anaconda3\\lib\\site-packages (from xgboost) (1.21.5)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-1.6.1\n"
     ]
    }
   ],
   "source": [
    "! pip install xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cc7277",
   "metadata": {},
   "source": [
    "## Importing Necessary Libraries\n",
    "\n",
    "### It is a good practice to import all the necessary libraries in one place — so that we can modify them quickly.\n",
    "\n",
    "### For this credit card data, the features that we have in the dataset are the transformed version of PCA, so we will not need to perform the feature selection again. Otherwise, it is recommended to use RFE, RFECV, SelectKBest and VIF score to find the best features for your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f14e0624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x216 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#Packages related to data importing, manipulation, exploratory data #analysis, data understanding\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "from termcolor import colored as cl # text customization\n",
    "#Packages related to data visualizaiton\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#Setting plot sizes and type of plot\n",
    "plt.rc(\"font\", size=14)\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.gray()\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.impute import MissingIndicator, SimpleImputer\n",
    "from sklearn.preprocessing import  PolynomialFeatures, KBinsDiscretizer, FunctionTransformer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, LabelBinarizer, OrdinalEncoder\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.tsa as tsa\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, ElasticNet, Lasso, Ridge\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, export_graphviz\n",
    "from sklearn.ensemble import BaggingClassifier, BaggingRegressor,RandomForestClassifier,RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingClassifier,GradientBoostingRegressor, AdaBoostClassifier, AdaBoostRegressor \n",
    "from sklearn.svm import LinearSVC, LinearSVR, SVC, SVR\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f197d616",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"creditcard.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5561136b",
   "metadata": {},
   "source": [
    "## Data Processing & Understanding\n",
    "\n",
    "### The one main thing you will notice about this data is that — the dataset is imbalanced towards a feature. Which seems pretty valid for such kind of data. Because today many banks have adopted different security mechanisms — so it is harder for hackers to make such moves.\n",
    "\n",
    "### Still, sometimes when there is some vulnerability in the system — the chance of such activities can increase.\n",
    "\n",
    "### That’s why we can see the majority of transactions belongs to our datasets are normal and only a few percentages of transactions are fraudulent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ec21af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mTotal number of Trnsactions are 284807\u001b[0m\n",
      "\u001b[1mNumber of Normal Transactions are 284315\u001b[0m\n",
      "\u001b[1mNumber of fraudulent Transactions are 492\u001b[0m\n",
      "\u001b[1mPercentage of fraud Transactions is 0.17\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "Total_transactions = len(data)\n",
    "normal = len(data[data.Class == 0])\n",
    "fraudulent = len(data[data.Class == 1])\n",
    "fraud_percentage = round(fraudulent/normal*100, 2)\n",
    "print(cl('Total number of Trnsactions are {}'.format(Total_transactions), attrs = ['bold']))\n",
    "print(cl('Number of Normal Transactions are {}'.format(normal), attrs = ['bold']))\n",
    "print(cl('Number of fraudulent Transactions are {}'.format(fraudulent), attrs = ['bold']))\n",
    "print(cl('Percentage of fraud Transactions is {}'.format(fraud_percentage), attrs = ['bold']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7521fc",
   "metadata": {},
   "source": [
    "## Only 0.17% of transactions are fraudulent.\n",
    "\n",
    "## We can also check for null values using the following line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49993919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 284807 entries, 0 to 284806\n",
      "Data columns (total 31 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   Time    284807 non-null  float64\n",
      " 1   V1      284807 non-null  float64\n",
      " 2   V2      284807 non-null  float64\n",
      " 3   V3      284807 non-null  float64\n",
      " 4   V4      284807 non-null  float64\n",
      " 5   V5      284807 non-null  float64\n",
      " 6   V6      284807 non-null  float64\n",
      " 7   V7      284807 non-null  float64\n",
      " 8   V8      284807 non-null  float64\n",
      " 9   V9      284807 non-null  float64\n",
      " 10  V10     284807 non-null  float64\n",
      " 11  V11     284807 non-null  float64\n",
      " 12  V12     284807 non-null  float64\n",
      " 13  V13     284807 non-null  float64\n",
      " 14  V14     284807 non-null  float64\n",
      " 15  V15     284807 non-null  float64\n",
      " 16  V16     284807 non-null  float64\n",
      " 17  V17     284807 non-null  float64\n",
      " 18  V18     284807 non-null  float64\n",
      " 19  V19     284807 non-null  float64\n",
      " 20  V20     284807 non-null  float64\n",
      " 21  V21     284807 non-null  float64\n",
      " 22  V22     284807 non-null  float64\n",
      " 23  V23     284807 non-null  float64\n",
      " 24  V24     284807 non-null  float64\n",
      " 25  V25     284807 non-null  float64\n",
      " 26  V26     284807 non-null  float64\n",
      " 27  V27     284807 non-null  float64\n",
      " 28  V28     284807 non-null  float64\n",
      " 29  Amount  284807 non-null  float64\n",
      " 30  Class   284807 non-null  int64  \n",
      "dtypes: float64(30), int64(1)\n",
      "memory usage: 67.4 MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04da38e",
   "metadata": {},
   "source": [
    "### As per the count per column, we have no null values. Also, feature selection is not the case for this use case. Anyway, you can try applying feature selection mechanisms to check if the results are optimised.\n",
    "\n",
    "### I have observed in our data 28 features are transformed versions of PCA but the Amount is the original one. And, while checking the minimum and maximum is in the amount — I found the difference is huge that can deviate our result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b15db041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 25691.16)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(data.Amount), max(data.Amount)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f492e47",
   "metadata": {},
   "source": [
    "## We have one more variable which is the time which can be an external deciding factor — but in our modelling process, we can drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a554a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "amount = data['Amount'].values\n",
    "data['Amount'] = sc.fit_transform(amount.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56553a7c",
   "metadata": {},
   "source": [
    "## We have one more variable which is the time which can be an external deciding factor — but in our modelling process, we can drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec9371ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['Time'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2743a7",
   "metadata": {},
   "source": [
    "## We can also check for any duplicate transactions. Before removing any duplicate transaction, we are having 284807 transactions in our data. Let’s remove the duplicate and observe the changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dfe836fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(284807, 30)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add475f8",
   "metadata": {},
   "source": [
    "### line of code to remove any duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae3cf8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33424e2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(275663, 30)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ce067c",
   "metadata": {},
   "source": [
    "### So, we were having around ~9000 duplicate transactions.\n",
    "\n",
    "### Here we go!! We now have properly scaled data with no duplicate, no missing. Let’s now split it for our model building."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91f222f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
